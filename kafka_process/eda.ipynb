{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "scala_version = '2.12'\n",
    "spark_version = '3.4.0'\n",
    "packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}',\n",
    "    'org.apache.kafka:kafka-clients:3.4.0'\n",
    "]\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"weather1\") \\\n",
    "        .config(\"spark.jars.packages\", \",\".join(packages))\\\n",
    "        .master(\"spark://10.245.211.187:7077\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StringType, StructField, IntegerType, TimestampType, FloatType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"date\", StringType()),\n",
    "    StructField(\"hour\", StringType()),\n",
    "    StructField(\"prcp\", FloatType()),\n",
    "    StructField(\"stp\", FloatType()),\n",
    "    StructField(\"smax\", FloatType()),\n",
    "    StructField(\"smin\", FloatType()),\n",
    "    StructField(\"gbrd\", FloatType()),\n",
    "    StructField(\"temp\", IntegerType()),\n",
    "    StructField(\"dewp\", FloatType()),\n",
    "    StructField(\"tmax\", FloatType()),\n",
    "    StructField(\"tmin\", FloatType()),\n",
    "    StructField(\"dmax\", FloatType()),\n",
    "    StructField(\"dmin\", FloatType()),\n",
    "    StructField(\"hmax\", IntegerType()),\n",
    "    StructField(\"hmin\", IntegerType()),\n",
    "    StructField(\"hmdy\", IntegerType()),\n",
    "    StructField(\"wdct\", IntegerType()),\n",
    "    StructField(\"gust\", StringType()),\n",
    "    StructField(\"wdsp\", StringType()),\n",
    "    StructField(\"region\", StringType()),\n",
    "    StructField(\"state\", StringType()),\n",
    "    StructField(\"latitude\", FloatType()),\n",
    "    StructField(\"longitude\", FloatType()),\n",
    "    StructField(\"height\", FloatType()),\n",
    "    #StructField(\"year\", IntegerType())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Kafka readStream\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"weather2\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "kafka_df.printSchema()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- prcp: float (nullable = true)\n",
      " |-- stp: float (nullable = true)\n",
      " |-- smax: float (nullable = true)\n",
      " |-- smin: float (nullable = true)\n",
      " |-- gbrd: float (nullable = true)\n",
      " |-- temp: integer (nullable = true)\n",
      " |-- dewp: float (nullable = true)\n",
      " |-- tmax: float (nullable = true)\n",
      " |-- tmin: float (nullable = true)\n",
      " |-- dmax: float (nullable = true)\n",
      " |-- dmin: float (nullable = true)\n",
      " |-- hmax: integer (nullable = true)\n",
      " |-- hmin: integer (nullable = true)\n",
      " |-- hmdy: integer (nullable = true)\n",
      " |-- wdct: integer (nullable = true)\n",
      " |-- gust: string (nullable = true)\n",
      " |-- wdsp: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- longitude: float (nullable = true)\n",
      " |-- height: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weatherdf = kafka_df.selectExpr(\"CAST(value AS STRING)\").select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "weatherdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "|key|value|topic|partition|offset|timestamp|timestampType|\n",
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_query = kafka_df.writeStream \\\n",
    "                .queryName('qdf') \\\n",
    "                .format('memory') \\\n",
    "                .start()\n",
    "\n",
    "raw = spark.sql(\"select * from qdf\")\n",
    "raw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+------+-----+--------+---------+------+\n",
      "|date|hour|prcp|stp|smax|smin|gbrd|temp|dewp|tmax|tmin|dmax|dmin|hmax|hmin|hmdy|wdct|gust|wdsp|region|state|latitude|longitude|height|\n",
      "+----+----+----+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+------+-----+--------+---------+------+\n",
      "+----+----+----+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+------+-----+--------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_query1 = weatherdf.writeStream \\\n",
    "                .queryName('qdf1') \\\n",
    "                .format('memory') \\\n",
    "                .start()\n",
    "\n",
    "raw1 = spark.sql(\"select * from qdf1\")\n",
    "raw1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherdf = weatherdf.selectExpr(\"to_json(struct(*)) AS value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_stream = weatherdf.writeStream \\\n",
    "      .outputMode(\"append\")\\\n",
    "      .format(\"console\")\\\n",
    "      .start() \n",
    "\n",
    "write_stream.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[region: string, count: bigint]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "query1 = kafka_df.groupBy(\"region\").agg(approx_count_distinct(\"date\").alias(\"count\"))\n",
    "print(query1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = query1.selectExpr().withColumn(\"value\", to_json(struct(\"*\")).cast(\"string\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = res.select(\"value\") \\\n",
    "        .writeStream \\\n",
    "        .outputMode(\"append\")  \\\n",
    "        .format(\"console\")  \\\n",
    "        .option(\"truncate\", \"False\")  \\\n",
    "        .start() \n",
    "res1.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+------+-----+--------+---------+------+\n",
      "|date|hour|prcp|stp|smax|smin|gbrd|temp|dewp|tmax|tmin|dmax|dmin|hmax|hmin|hmdy|wdct|gust|wdsp|region|state|latitude|longitude|height|\n",
      "+----+----+----+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+------+-----+--------+---------+------+\n",
      "+----+----+----+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+------+-----+--------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = spark.sql(\"select * from qdf\")\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "ds_kafka = (\n",
    "        kafka_df.selectExpr(\"CAST(value AS STRING)\")\n",
    "        .select(F.from_json(\"value\", schema=schema).alias(\"data\"))\n",
    "        .select(\"data.*\")\n",
    "        # .coalesce(1)\n",
    "        # .writeStream.queryName(\"streaming_to_console\")\n",
    "        # .trigger(processingTime=\"2 minute\")\n",
    "        # .outputMode(\"complete\")\n",
    "        # .format(\"console\")\n",
    "        # .option(\"truncate\", False)\n",
    "        # .start()\n",
    "    )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_kafka.writeStream \\\n",
    "      .outputMode(\"append\")  \\\n",
    "      .format(\"console\")  \\\n",
    "      .option(\"truncate\", \"False\")  \\\n",
    "      .start() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = kafka_df.selectExpr(\"CAST(value AS STRING)\")\n",
    "print(type(kafka_df))\n",
    "print(type(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query = kafka_df.writeStream \\\n",
    "                .queryName('qdf') \\\n",
    "                .format('memory') \\\n",
    "                .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alert = ds.writeStream \\\n",
    "            .queryName('qalerts') \\\n",
    "            .format('memory') \\\n",
    "            .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alerts = spark.sql(\"select * from qalerts\")\n",
    "alerts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddAleartsRdd = alerts.rdd.map(lambda alert: literal_eval(alert['value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddAleartsRdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddAlerts = rddAleartsRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(rddAlerts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddAlerts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
